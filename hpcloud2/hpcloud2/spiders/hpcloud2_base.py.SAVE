# -*- coding: utf-8 -*-#
from __future__ import absolute_import

from scrapy.selector import HtmlXPathSelector
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule

from scrapy.http import FormRequest, Request
from scrapy.utils.response import open_in_browser
import urlparse

from hpcloud2.items import Hpcloud2Item


class Hpcloud2Spider(CrawlSpider):
    name = 'hpcloud2'
    #allowed_domains = ['console.hpcloud.com/login']
    start_urls = ['https://console.hpcloud.com/invoices?year=2012']
 
    rules = (
        Rule(SgmlLinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def __init__(self, *args, **kwargs):
        super(Hpcloud2Spider, self).__init__(*args, **kwargs)
        self.username = 'sudhi@hooduku.com'
        self.password = 'Java0man'

    def parse_item(self, response):
        hxs = HtmlXPathSelector(response)
        #i = Hpcloud2Item()
        #i['domain_id'] = hxs.select('//input[@id="sid"]/@value').extract()
        #i['name'] = hxs.select('//div[@id="name"]').extract()
        #i['description'] = hxs.select('//div[@id="description"]').extract()
        auth_token = hxs.select('//input[@name="authenticity_token"]/@value').extract()
        if auth_token:
            return [FormRequest.from_response(response, formnumber=0,
                        formdata={"user[username]": self.username, "user[password]": self.password,
                            "authenticity_token":auth_token,"utf8":u"âœ“"
                            },
                        callback=self.after_login)]

    def after_login(self, response):
        hxs = HtmlXPathSelector(response)
        alert = hxs.select('//ul[@class="message-alert"]').extract()
        if alert :
            print "Invalid login"
            print alert
            return
        #print "Login ok!"
        item_url = "https://console.hpcloud.com/invoices?year=2012"
        #print "GOTO",item_url
        yield Request(url=item_url, callback=self.parse_bills_list)

    def parse_bills_list(self,response):
        print "dir=",dir(response)
        hxs = HtmlXPathSelector(response)
        allrefs = hxs.select('//td[@class="shrink"]/a')

        allrefs = allrefs[:1]       # ONE OBJECT FOR DEBUG

        for refs in  allrefs:
            #print refs
            href = refs.select('@href').extract()[0]
            print "extract data=",href
            print "extract text=",refs.select('text()').extract()
            #yield Request(url=urlparse.urljoin(self.start_urls[0],href), callback=self.parse_bill)
            yield Request(url=urlparse.urljoin(response.url,href), callback=self.parse_bill)

    def parse_bill(self, response):
        print "PARSE_BILL!"
        open_in_browser(response)
